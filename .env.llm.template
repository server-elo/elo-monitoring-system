# Local LLM Configuration - Add to your .env.local

# Primary LLM Services (Your 3 running instances)
CODE_LLM_URL=http://localhost:1234/v1
CODE_LLM_API_KEY=lm-studio-code

CHAT_LLM_URL=http://localhost:1235/v1  
CHAT_LLM_API_KEY=lm-studio-chat

FAST_LLM_URL=http://localhost:1236/v1
FAST_LLM_API_KEY=lm-studio-fast

# Fallback (keep your existing Gemini)
GEMINI_API_KEY=your_existing_gemini_key

# LLM Routing Configuration
ENABLE_MULTI_LLM=true
DEFAULT_CODE_MODEL=codellama-34b
DEFAULT_CHAT_MODEL=mixtral-8x7b
DEFAULT_FAST_MODEL=llama-3.1-8b

# Performance Settings
LLM_TIMEOUT=30000
LLM_MAX_TOKENS=4096
LLM_TEMPERATURE=0.1

# Service Health Check
LLM_HEALTH_CHECK_INTERVAL=60000
LLM_AUTO_FALLBACK=true
